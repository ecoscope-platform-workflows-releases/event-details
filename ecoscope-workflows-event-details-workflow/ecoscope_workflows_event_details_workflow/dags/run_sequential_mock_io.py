# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details

# ruff: noqa: E402

"""WARNING: This file is generated in a testing context and should not be used in production.
Lines specific to the testing context are marked with a test tube emoji (ðŸ§ª) to indicate
that they would not be included (or would be different) in the production version of this file.
"""

import json
import os
import warnings  # ðŸ§ª

from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import (
    get_timezone_from_time_range as get_timezone_from_time_range,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_is_none as any_dependency_is_none,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.testing import create_task_magicmock  # ðŸ§ª
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_analysis_field_from_event_details as get_analysis_field_from_event_details,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_analysis_field_label_from_event_details as get_analysis_field_label_from_event_details,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_analysis_field_unit_from_event_details as get_analysis_field_unit_from_event_details,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_category_field_from_event_details as get_category_field_from_event_details,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_category_field_label_from_event_details as get_category_field_label_from_event_details,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_event_type_from_event_details as get_event_type_from_event_details,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    set_event_details_params as set_event_details_params,
)

get_events_from_combined_params = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_events_from_combined_params",  # ðŸ§ª
)  # ðŸ§ª

get_fields_from_event_type_schema = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_fields_from_event_type_schema",  # ðŸ§ª
)  # ðŸ§ª

get_choices_from_v2_event_type = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_choices_from_v2_event_type",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.transformation import (
    convert_values_to_timezone as convert_values_to_timezone,
)
from ecoscope_workflows_core.tasks.transformation import (
    extract_value_from_json_column as extract_value_from_json_column,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    extract_spatial_grouper_feature_group_names as extract_spatial_grouper_feature_group_names,
)

get_spatial_features_group = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_spatial_features_group",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.analysis import dataframe_count as dataframe_count
from ecoscope_workflows_core.tasks.config import (
    concat_string_vars as concat_string_vars,
)
from ecoscope_workflows_core.tasks.config import (
    default_if_string_is_empty as default_if_string_is_empty,
)
from ecoscope_workflows_core.tasks.config import (
    get_column_names_from_dataframe as get_column_names_from_dataframe,
)
from ecoscope_workflows_core.tasks.config import set_string_var as set_string_var
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_plot_widget_single_view as create_plot_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_single_value_widget_single_view as create_single_value_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_table_widget_single_view as create_table_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_is_empty_string as any_dependency_is_empty_string,
)
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import assign_value as assign_value
from ecoscope_workflows_core.tasks.transformation import (
    convert_column_values_to_numeric as convert_column_values_to_numeric,
)
from ecoscope_workflows_core.tasks.transformation import fill_na as fill_na
from ecoscope_workflows_core.tasks.transformation import (
    lookup_string_var as lookup_string_var,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_core.tasks.transformation import map_values as map_values
from ecoscope_workflows_core.tasks.transformation import (
    reorder_columns as reorder_columns,
)
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_core.tasks.transformation import (
    strip_prefix_from_column_names as strip_prefix_from_column_names,
)
from ecoscope_workflows_core.tasks.transformation import transpose as transpose
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_feature_density as calculate_feature_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    create_meshgrid as create_meshgrid,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import summarize_df as summarize_df
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    create_point_layer as create_point_layer,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    create_polygon_layer as create_polygon_layer,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_ecomap as draw_ecomap
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    draw_pie_chart as draw_pie_chart,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_table as draw_table
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    draw_time_series_bar_chart as draw_time_series_bar_chart,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import set_base_maps as set_base_maps
from ecoscope_workflows_ext_ecoscope.tasks.skip import (
    all_geometry_are_none as all_geometry_are_none,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    add_spatial_index as add_spatial_index,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification as apply_classification,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_reloc_coord_filter as apply_reloc_coord_filter,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    drop_nan_values_by_column as drop_nan_values_by_column,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    drop_null_geometry as drop_null_geometry,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    normalize_json_column as normalize_json_column,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    normalize_numeric_column as normalize_numeric_column,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    resolve_spatial_feature_groups_for_spatial_groupers as resolve_spatial_feature_groups_for_spatial_groupers,
)

from ..params import Params


def main(params: Params):
    warnings.warn("This test script should not be used in production!")  # ðŸ§ª

    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_format="%d %b %Y %H:%M:%S", **(params_dict.get("time_range") or {})
        )
        .call()
    )

    get_timezone = (
        get_timezone_from_time_range.validate()
        .set_task_instance_id("get_timezone")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(time_range=time_range, **(params_dict.get("get_timezone") or {}))
        .call()
    )

    set_event_details_combined = (
        set_event_details_params.validate()
        .set_task_instance_id("set_event_details_combined")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            event_columns=[
                "id",
                "time",
                "event_type",
                "event_category",
                "reported_by",
                "serial_number",
                "location",
                "event_details",
                "geometry",
            ],
            raise_on_empty=False,
            include_details=True,
            include_updates=False,
            include_related_events=False,
            include_display_values=False,
            **(params_dict.get("set_event_details_combined") or {}),
        )
        .call()
    )

    analysis_field = (
        get_analysis_field_from_event_details.validate()
        .set_task_instance_id("analysis_field")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            combined_params=set_event_details_combined,
            **(params_dict.get("analysis_field") or {}),
        )
        .call()
    )

    analysis_field_label = (
        get_analysis_field_label_from_event_details.validate()
        .set_task_instance_id("analysis_field_label")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            combined_params=set_event_details_combined,
            **(params_dict.get("analysis_field_label") or {}),
        )
        .call()
    )

    analysis_field_unit = (
        get_analysis_field_unit_from_event_details.validate()
        .set_task_instance_id("analysis_field_unit")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            combined_params=set_event_details_combined,
            **(params_dict.get("analysis_field_unit") or {}),
        )
        .call()
    )

    category_field = (
        get_category_field_from_event_details.validate()
        .set_task_instance_id("category_field")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_dependency_is_none,
            ],
            unpack_depth=1,
        )
        .partial(
            combined_params=set_event_details_combined,
            **(params_dict.get("category_field") or {}),
        )
        .call()
    )

    category_field_label = (
        get_category_field_label_from_event_details.validate()
        .set_task_instance_id("category_field_label")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_dependency_is_none,
            ],
            unpack_depth=1,
        )
        .partial(
            combined_params=set_event_details_combined,
            **(params_dict.get("category_field_label") or {}),
        )
        .call()
    )

    event_type = (
        get_event_type_from_event_details.validate()
        .set_task_instance_id("event_type")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            combined_params=set_event_details_combined,
            **(params_dict.get("event_type") or {}),
        )
        .call()
    )

    get_events_data = (
        get_events_from_combined_params.validate()
        .set_task_instance_id("get_events_data")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            combined_params=set_event_details_combined,
            **(params_dict.get("get_events_data") or {}),
        )
        .call()
    )

    get_event_schema_display_names = (
        get_fields_from_event_type_schema.validate()
        .set_task_instance_id("get_event_schema_display_names")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            event_type=event_type,
            **(params_dict.get("get_event_schema_display_names") or {}),
        )
        .call()
    )

    get_category_display_names = (
        get_choices_from_v2_event_type.validate()
        .set_task_instance_id("get_category_display_names")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            event_type=event_type,
            choice_field=category_field,
            **(params_dict.get("get_category_display_names") or {}),
        )
        .call()
    )

    convert_to_user_timezone = (
        convert_values_to_timezone.validate()
        .set_task_instance_id("convert_to_user_timezone")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=get_events_data,
            timezone=get_timezone,
            columns=["time"],
            **(params_dict.get("convert_to_user_timezone") or {}),
        )
        .call()
    )

    extract_latitude = (
        extract_value_from_json_column.validate()
        .set_task_instance_id("extract_latitude")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=convert_to_user_timezone,
            column_name="location",
            field_name_options=["latitude"],
            output_type="str",
            output_column_name="latitude",
            **(params_dict.get("extract_latitude") or {}),
        )
        .call()
    )

    extract_longitude = (
        extract_value_from_json_column.validate()
        .set_task_instance_id("extract_longitude")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=extract_latitude,
            column_name="location",
            field_name_options=["longitude"],
            output_type="str",
            output_column_name="longitude",
            **(params_dict.get("extract_longitude") or {}),
        )
        .call()
    )

    extract_reported_by = (
        extract_value_from_json_column.validate()
        .set_task_instance_id("extract_reported_by")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=extract_longitude,
            column_name="reported_by",
            field_name_options=["name"],
            output_type="str",
            output_column_name="reported_by_name",
            **(params_dict.get("extract_reported_by") or {}),
        )
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("groupers") or {}))
        .call()
    )

    spatial_group_ids = (
        extract_spatial_grouper_feature_group_names.validate()
        .set_task_instance_id("spatial_group_ids")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(groupers=groupers, **(params_dict.get("spatial_group_ids") or {}))
        .call()
    )

    fetch_all_spatial_feature_groups = (
        get_spatial_features_group.validate()
        .set_task_instance_id("fetch_all_spatial_feature_groups")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            **(params_dict.get("fetch_all_spatial_feature_groups") or {}),
        )
        .map(argnames=["spatial_features_group_name"], argvalues=spatial_group_ids)
    )

    resolved_groupers = (
        resolve_spatial_feature_groups_for_spatial_groupers.validate()
        .set_task_instance_id("resolved_groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            groupers=groupers,
            spatial_feature_groups=fetch_all_spatial_feature_groups,
            **(params_dict.get("resolved_groupers") or {}),
        )
        .call()
    )

    filter_events = (
        apply_reloc_coord_filter.validate()
        .set_task_instance_id("filter_events")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=extract_reported_by,
            roi_gdf=None,
            roi_name=None,
            reset_index=True,
            **(params_dict.get("filter_events") or {}),
        )
        .call()
    )

    normalize_event_details = (
        normalize_json_column.validate()
        .set_task_instance_id("normalize_event_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=filter_events,
            column="event_details",
            skip_if_not_exists=False,
            sort_columns=False,
            **(params_dict.get("normalize_event_details") or {}),
        )
        .call()
    )

    strip_event_details_prefix = (
        strip_prefix_from_column_names.validate()
        .set_task_instance_id("strip_event_details_prefix")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            prefix="event_details__",
            df=normalize_event_details,
            **(params_dict.get("strip_event_details_prefix") or {}),
        )
        .call()
    )

    events_add_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("events_add_temporal_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=strip_event_details_prefix,
            time_col="time",
            groupers=resolved_groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("events_add_temporal_index") or {}),
        )
        .call()
    )

    events_add_spatial_index = (
        add_spatial_index.validate()
        .set_task_instance_id("events_add_spatial_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=events_add_temporal_index,
            groupers=resolved_groupers,
            **(params_dict.get("events_add_spatial_index") or {}),
        )
        .call()
    )

    add_default_category_column = (
        assign_value.validate()
        .set_task_instance_id("add_default_category_column")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=events_add_spatial_index,
            column_name="default_category",
            value=analysis_field_unit,
            noop_if_column_exists=False,
            **(params_dict.get("add_default_category_column") or {}),
        )
        .call()
    )

    default_category_field = (
        default_if_string_is_empty.validate()
        .set_task_instance_id("default_category_field")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            value=category_field,
            default="default_category",
            **(params_dict.get("default_category_field") or {}),
        )
        .call()
    )

    category_field_label_or_category = (
        default_if_string_is_empty.validate()
        .set_task_instance_id("category_field_label_or_category")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            value=category_field_label,
            default=category_field,
            **(params_dict.get("category_field_label_or_category") or {}),
        )
        .call()
    )

    default_category_field_label = (
        default_if_string_is_empty.validate()
        .set_task_instance_id("default_category_field_label")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            value=category_field_label_or_category,
            default=category_field,
            **(params_dict.get("default_category_field_label") or {}),
        )
        .call()
    )

    ensure_analysis_column = (
        assign_value.validate()
        .set_task_instance_id("ensure_analysis_column")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=add_default_category_column,
            column_name=analysis_field,
            value=None,
            noop_if_column_exists=True,
            **(params_dict.get("ensure_analysis_column") or {}),
        )
        .call()
    )

    ensure_category_column = (
        assign_value.validate()
        .set_task_instance_id("ensure_category_column")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=ensure_analysis_column,
            column_name=default_category_field,
            value="None",
            noop_if_column_exists=True,
            **(params_dict.get("ensure_category_column") or {}),
        )
        .call()
    )

    analysis_field_display_name = (
        lookup_string_var.validate()
        .set_task_instance_id("analysis_field_display_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var=analysis_field,
            value_map=get_event_schema_display_names,
            raise_if_not_found=True,
            **(params_dict.get("analysis_field_display_name") or {}),
        )
        .call()
    )

    category_field_display_name = (
        lookup_string_var.validate()
        .set_task_instance_id("category_field_display_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var=default_category_field,
            value_map=get_event_schema_display_names,
            raise_if_not_found=False,
            **(params_dict.get("category_field_display_name") or {}),
        )
        .call()
    )

    map_display_names = (
        map_values.validate()
        .set_task_instance_id("map_display_names")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=ensure_category_column,
            column_name=default_category_field,
            value_map=get_category_display_names,
            missing_values="preserve",
            replacement=None,
            **(params_dict.get("map_display_names") or {}),
        )
        .call()
    )

    convert_na_values = (
        fill_na.validate()
        .set_task_instance_id("convert_na_values")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=map_display_names,
            value="None",
            columns=[default_category_field],
            **(params_dict.get("convert_na_values") or {}),
        )
        .call()
    )

    rename_columns = (
        map_columns.validate()
        .set_task_instance_id("rename_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=convert_na_values,
            drop_columns=["reported_by", "location", "updates"],
            retain_columns=[],
            rename_columns={
                "serial_number": "Serial Number",
                "time": "Event Time",
                "reported_by_name": "Reported By",
                "latitude": "Latitude",
                "longitude": "Longitude",
                "event_category": "Event Category",
            },
            raise_if_not_found=True,
            **(params_dict.get("rename_columns") or {}),
        )
        .call()
    )

    column_display_order = (
        reorder_columns.validate()
        .set_task_instance_id("column_display_order")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_columns,
            columns=[
                "Serial Number",
                "Event Time",
                "Reported By",
                "Latitude",
                "Longitude",
            ],
            **(params_dict.get("column_display_order") or {}),
        )
        .call()
    )

    events_table_columns = (
        get_column_names_from_dataframe.validate()
        .set_task_instance_id("events_table_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=column_display_order,
            exclude_column_names=[
                "id",
                "geometry",
                "events_colormap",
                "event_type",
                "default_category",
                "Event Category",
            ],
            **(params_dict.get("events_table_columns") or {}),
        )
        .call()
    )

    events_table_display_columns = (
        lookup_string_var.validate()
        .set_task_instance_id("events_table_display_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            value_map=get_event_schema_display_names,
            raise_if_not_found=False,
            **(params_dict.get("events_table_display_columns") or {}),
        )
        .map(argnames=["var"], argvalues=events_table_columns)
    )

    ensure_numeric = (
        convert_column_values_to_numeric.validate()
        .set_task_instance_id("ensure_numeric")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_columns,
            columns=[analysis_field],
            **(params_dict.get("ensure_numeric") or {}),
        )
        .call()
    )

    events_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("events_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=ensure_numeric,
            input_column_name=default_category_field,
            colormap=[
                "#312B86",
                "#08D1CC",
                "#FF3FB3",
                "#20BF55",
                "#FF9F1C",
                "#6460B0",
                "#FF7FCC",
                "#4DE3DD",
                "#6BE39A",
                "#FFC46F",
                "#1E215C",
                "#C80088",
                "#008784",
                "#08853A",
                "#C46C00",
            ],
            output_column_name="events_colormap",
            **(params_dict.get("events_colormap") or {}),
        )
        .call()
    )

    set_summary_table_title = (
        set_string_var.validate()
        .set_task_instance_id("set_summary_table_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var=analysis_field_label,
            **(params_dict.get("set_summary_table_title") or {}),
        )
        .call()
    )

    by_category_field_str = (
        concat_string_vars.validate()
        .set_task_instance_id("by_category_field_str")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_dependency_is_empty_string,
            ],
            unpack_depth=1,
        )
        .partial(
            values=[" by ", category_field_label],
            **(params_dict.get("by_category_field_str") or {}),
        )
        .call()
    )

    pie_chart_title_pt1 = (
        concat_string_vars.validate()
        .set_task_instance_id("pie_chart_title_pt1")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            values=["Proportion of ", analysis_field_label],
            **(params_dict.get("pie_chart_title_pt1") or {}),
        )
        .call()
    )

    set_pie_chart_title = (
        concat_string_vars.validate()
        .set_task_instance_id("set_pie_chart_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            values=[pie_chart_title_pt1, by_category_field_str],
            **(params_dict.get("set_pie_chart_title") or {}),
        )
        .call()
    )

    set_events_map_title = (
        concat_string_vars.validate()
        .set_task_instance_id("set_events_map_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            values=[analysis_field_label, by_category_field_str, " Locations"],
            **(params_dict.get("set_events_map_title") or {}),
        )
        .call()
    )

    set_bar_chart_title = (
        concat_string_vars.validate()
        .set_task_instance_id("set_bar_chart_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            values=[analysis_field_label, by_category_field_str, " Over Time"],
            **(params_dict.get("set_bar_chart_title") or {}),
        )
        .call()
    )

    set_sum_map_title = (
        concat_string_vars.validate()
        .set_task_instance_id("set_sum_map_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            values=[analysis_field_label, " Sum"],
            **(params_dict.get("set_sum_map_title") or {}),
        )
        .call()
    )

    set_events_table_title = (
        set_string_var.validate()
        .set_task_instance_id("set_events_table_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var="Events Table", **(params_dict.get("set_events_table_title") or {})
        )
        .call()
    )

    split_event_groups = (
        split_groups.validate()
        .set_task_instance_id("split_event_groups")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=events_colormap,
            groupers=resolved_groupers,
            **(params_dict.get("split_event_groups") or {}),
        )
        .call()
    )

    display_table = (
        map_columns.validate()
        .set_task_instance_id("display_table")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns=get_event_schema_display_names,
            raise_if_not_found=False,
            **(params_dict.get("display_table") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_event_groups)
    )

    drop_nan_values = (
        drop_nan_values_by_column.validate()
        .set_task_instance_id("drop_nan_values")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name=analysis_field_display_name,
            **(params_dict.get("drop_nan_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=display_table)
    )

    base_map_defs = (
        set_base_maps.validate()
        .set_task_instance_id("base_map_defs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("base_map_defs") or {}))
        .call()
    )

    total_events = (
        dataframe_count.validate()
        .set_task_instance_id("total_events")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("total_events") or {}))
        .mapvalues(argnames=["df"], argvalues=display_table)
    )

    total_events_sv_widget = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_events_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Event Count",
            decimal_places=1,
            **(params_dict.get("total_events_sv_widget") or {}),
        )
        .map(argnames=["view", "data"], argvalues=total_events)
    )

    total_events_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_events_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_events_sv_widget,
            **(params_dict.get("total_events_grouped_sv_widget") or {}),
        )
        .call()
    )

    grouped_event_summary = (
        summarize_df.validate()
        .set_task_instance_id("grouped_event_summary")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            summary_params=[
                {
                    "display_name": "Sum",
                    "aggregator": "sum",
                    "column": analysis_field_display_name,
                },
                {
                    "display_name": "Min",
                    "aggregator": "min",
                    "column": analysis_field_display_name,
                },
                {
                    "display_name": "Max",
                    "aggregator": "max",
                    "column": analysis_field_display_name,
                },
                {
                    "display_name": "Median",
                    "aggregator": "median",
                    "column": analysis_field_display_name,
                },
                {
                    "display_name": "Mean",
                    "aggregator": "mean",
                    "column": analysis_field_display_name,
                },
            ],
            groupby_cols=None,
            reset_index=False,
            **(params_dict.get("grouped_event_summary") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=drop_nan_values)
    )

    transpose_table = (
        transpose.validate()
        .set_task_instance_id("transpose_table")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            transposed_column_name="Summary Stats",
            **(params_dict.get("transpose_table") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=grouped_event_summary)
    )

    rename_summary_columns = (
        map_columns.validate()
        .set_task_instance_id("rename_summary_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={"0": "Summary Values"},
            raise_if_not_found=True,
            **(params_dict.get("rename_summary_columns") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=transpose_table)
    )

    summary_table = (
        draw_table.validate()
        .set_task_instance_id("summary_table")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=["Summary Stats", "Summary Values"],
            table_config={
                "enable_sorting": True,
                "enable_filtering": False,
                "enable_download": True,
                "hide_header": True,
            },
            widget_id=set_summary_table_title,
            **(params_dict.get("summary_table") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=rename_summary_columns)
    )

    summary_html_urls = (
        persist_text.validate()
        .set_task_instance_id("summary_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("summary_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=summary_table)
    )

    summary_table_single_views = (
        create_table_widget_single_view.validate()
        .set_task_instance_id("summary_table_single_views")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_summary_table_title,
            **(params_dict.get("summary_table_single_views") or {}),
        )
        .map(argnames=["view", "data"], argvalues=summary_html_urls)
    )

    grouped_summary_table_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_summary_table_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=summary_table_single_views,
            **(params_dict.get("grouped_summary_table_widget") or {}),
        )
        .call()
    )

    grouped_events_pie_chart = (
        draw_pie_chart.validate()
        .set_task_instance_id("grouped_events_pie_chart")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            value_column=analysis_field_display_name,
            color_column="events_colormap",
            plot_style={"textinfo": "value"},
            label_column=category_field_display_name,
            layout_style=None,
            widget_id=set_pie_chart_title,
            **(params_dict.get("grouped_events_pie_chart") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=drop_nan_values)
    )

    grouped_pie_chart_html_urls = (
        persist_text.validate()
        .set_task_instance_id("grouped_pie_chart_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("grouped_pie_chart_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=grouped_events_pie_chart)
    )

    grouped_events_pie_chart_widgets = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("grouped_events_pie_chart_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_pie_chart_title,
            **(params_dict.get("grouped_events_pie_chart_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=grouped_pie_chart_html_urls)
    )

    grouped_events_pie_widget_merge = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_events_pie_widget_merge")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=grouped_events_pie_chart_widgets,
            **(params_dict.get("grouped_events_pie_widget_merge") or {}),
        )
        .call()
    )

    normalize_analysis_field = (
        normalize_numeric_column.validate()
        .set_task_instance_id("normalize_analysis_field")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column=analysis_field_display_name,
            output_column_name="normalized_analysis_field",
            **(params_dict.get("normalize_analysis_field") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=display_table)
    )

    drop_empty_geometry = (
        drop_null_geometry.validate()
        .set_task_instance_id("drop_empty_geometry")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("drop_empty_geometry") or {}))
        .mapvalues(argnames=["gdf"], argvalues=normalize_analysis_field)
    )

    grouped_events_map_layer = (
        create_point_layer.validate()
        .set_task_instance_id("grouped_events_map_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "events_colormap",
                "get_radius": "normalized_analysis_field",
                "stroked": True,
                "get_line_color": "#FFFFFF",
                "radius_scale": 5,
            },
            legend={
                "label_column": category_field_display_name,
                "color_column": "events_colormap",
            },
            tooltip_columns=[
                "Serial Number",
                "Event Time",
                "Reported By",
                analysis_field_display_name,
            ],
            **(params_dict.get("grouped_events_map_layer") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=drop_empty_geometry)
    )

    grouped_events_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("grouped_events_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title=None,
            tile_layers=base_map_defs,
            north_arrow_style={"placement": "top-left"},
            legend_style={
                "title": default_category_field_label,
                "format_title": False,
                "placement": "bottom-right",
            },
            static=False,
            max_zoom=20,
            widget_id=set_events_map_title,
            **(params_dict.get("grouped_events_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers"], argvalues=grouped_events_map_layer)
    )

    grouped_events_ecomap_html_url = (
        persist_text.validate()
        .set_task_instance_id("grouped_events_ecomap_html_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("grouped_events_ecomap_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=grouped_events_ecomap)
    )

    grouped_events_map_widget = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("grouped_events_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_events_map_title,
            **(params_dict.get("grouped_events_map_widget") or {}),
        )
        .map(argnames=["view", "data"], argvalues=grouped_events_ecomap_html_url)
    )

    grouped_events_map_widget_merge = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_events_map_widget_merge")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=grouped_events_map_widget,
            **(params_dict.get("grouped_events_map_widget_merge") or {}),
        )
        .call()
    )

    events_bar_chart = (
        draw_time_series_bar_chart.validate()
        .set_task_instance_id("events_bar_chart")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            x_axis="Event Time",
            y_axis=analysis_field_display_name,
            category=category_field_display_name,
            agg_function="sum",
            color_column="events_colormap",
            plot_style={"xperiodalignment": "middle"},
            layout_style=None,
            widget_id=set_bar_chart_title,
            **(params_dict.get("events_bar_chart") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=drop_nan_values)
    )

    events_bar_chart_html_url = (
        persist_text.validate()
        .set_task_instance_id("events_bar_chart_html_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("events_bar_chart_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=events_bar_chart)
    )

    events_bar_chart_widget = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("events_bar_chart_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_bar_chart_title,
            **(params_dict.get("events_bar_chart_widget") or {}),
        )
        .map(argnames=["view", "data"], argvalues=events_bar_chart_html_url)
    )

    grouped_bar_plot_widget_merge = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_bar_plot_widget_merge")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=events_bar_chart_widget,
            **(params_dict.get("grouped_bar_plot_widget_merge") or {}),
        )
        .call()
    )

    events_meshgrid = (
        create_meshgrid.validate()
        .set_task_instance_id("events_meshgrid")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            aoi=events_add_spatial_index,
            intersecting_only=False,
            **(params_dict.get("events_meshgrid") or {}),
        )
        .call()
    )

    grouped_events_sum_map = (
        calculate_feature_density.validate()
        .set_task_instance_id("grouped_events_sum_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            meshgrid=events_meshgrid,
            geometry_type="point",
            sum_column=analysis_field_display_name,
            **(params_dict.get("grouped_events_sum_map") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=display_table)
    )

    drop_nan_percentiles = (
        drop_nan_values_by_column.validate()
        .set_task_instance_id("drop_nan_percentiles")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="density", **(params_dict.get("drop_nan_percentiles") or {})
        )
        .mapvalues(argnames=["df"], argvalues=grouped_events_sum_map)
    )

    sort_grouped_sum_values = (
        sort_values.validate()
        .set_task_instance_id("sort_grouped_sum_values")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="density",
            ascending=True,
            na_position="last",
            **(params_dict.get("sort_grouped_sum_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=drop_nan_percentiles)
    )

    classify_fd = (
        apply_classification.validate()
        .set_task_instance_id("classify_fd")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="density",
            output_column_name="density_bins",
            classification_options={"scheme": "equal_interval", "k": 9},
            label_options={"label_ranges": True, "label_decimals": 0},
            **(params_dict.get("classify_fd") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_grouped_sum_values)
    )

    grouped_fd_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("grouped_fd_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="density_bins",
            colormap=[
                "#5C4FE7",
                "#269CD5",
                "#00D0C8",
                "#81D670",
                "#FFD000",
                "#F9BB12",
                "#FF9600",
                "#FA7306",
                "#F23B0E",
            ],
            output_column_name="density_colormap",
            **(params_dict.get("grouped_fd_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=classify_fd)
    )

    fd_rename_columns = (
        map_columns.validate()
        .set_task_instance_id("fd_rename_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={"density": "Sum"},
            raise_if_not_found=True,
            **(params_dict.get("fd_rename_columns") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=grouped_fd_colormap)
    )

    grouped_fd_map_layer = (
        create_polygon_layer.validate()
        .set_task_instance_id("grouped_fd_map_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "density_colormap",
                "get_line_width": 0,
                "opacity": 0.4,
            },
            legend={"label_column": "density_bins", "color_column": "density_colormap"},
            tooltip_columns=["Sum"],
            **(params_dict.get("grouped_fd_map_layer") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=fd_rename_columns)
    )

    grouped_fd_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("grouped_fd_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title=None,
            tile_layers=base_map_defs,
            north_arrow_style={"placement": "top-left"},
            legend_style={
                "title": analysis_field_unit,
                "format_title": False,
                "placement": "bottom-right",
            },
            static=False,
            max_zoom=20,
            widget_id=set_sum_map_title,
            **(params_dict.get("grouped_fd_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers"], argvalues=grouped_fd_map_layer)
    )

    grouped_fd_ecomap_html_url = (
        persist_text.validate()
        .set_task_instance_id("grouped_fd_ecomap_html_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("grouped_fd_ecomap_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=grouped_fd_ecomap)
    )

    grouped_fd_map_widget = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("grouped_fd_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_sum_map_title, **(params_dict.get("grouped_fd_map_widget") or {})
        )
        .map(argnames=["view", "data"], argvalues=grouped_fd_ecomap_html_url)
    )

    grouped_fd_map_widget_merge = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_fd_map_widget_merge")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=grouped_fd_map_widget,
            **(params_dict.get("grouped_fd_map_widget_merge") or {}),
        )
        .call()
    )

    events_table = (
        draw_table.validate()
        .set_task_instance_id("events_table")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=events_table_display_columns,
            table_config={
                "enable_sorting": True,
                "enable_filtering": False,
                "enable_download": True,
                "hide_header": False,
            },
            widget_id=set_events_table_title,
            **(params_dict.get("events_table") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=display_table)
    )

    table_html_urls = (
        persist_text.validate()
        .set_task_instance_id("table_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("table_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=events_table)
    )

    events_table_single_views = (
        create_table_widget_single_view.validate()
        .set_task_instance_id("events_table_single_views")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_events_table_title,
            **(params_dict.get("events_table_single_views") or {}),
        )
        .map(argnames=["view", "data"], argvalues=table_html_urls)
    )

    grouped_table_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_table_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=events_table_single_views,
            **(params_dict.get("grouped_table_widget") or {}),
        )
        .call()
    )

    events_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("events_dashboard")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=workflow_details,
            widgets=[
                total_events_grouped_sv_widget,
                grouped_summary_table_widget,
                grouped_events_pie_widget_merge,
                grouped_events_map_widget_merge,
                grouped_bar_plot_widget_merge,
                grouped_fd_map_widget_merge,
                grouped_table_widget,
            ],
            groupers=resolved_groupers,
            time_range=time_range,
            **(params_dict.get("events_dashboard") or {}),
        )
        .call()
    )

    return events_dashboard
